# -*- coding: utf-8 -*-
"""ensemble-classifier-detecting-fraud-transactions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wUBnn1ZQc-nnp60Mkmd2d7GuD063RTLt

## Importing the packages
"""

import numpy as np
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier

"""## Importing the Dataset"""

df = pd.read_csv('Fraud_transaction.csv')

df.head()

df.shape

"""## Overview of the Data"""

def missing_info(column, df):

    '''
        Utility Function to compute the missing value info
        of any feature in a dataset.
    '''

    na = df[column].isna()
    count = na.sum()
    total_count = df.shape[0]
    miss_prcnt = np.round((count/total_count)*100,3)

    return (count, miss_prcnt)

def missing_train_info(df):
    '''
        Utility Function to get information about missing values in the dataframe,
        for each column the following information is given by this function.

            - Number of missing values present in the column.
            - Percentage of missing values in the column.
    '''

    columns_missing_info = []

    for column in df:

        count, miss_prcnt = missing_info(column, df);

        if(count):
            columns_missing_info.append([column, count, miss_prcnt])

    column_names = ['Feature_Name', 'Missing_Count', 'Missing_Percentage']

    missing_info_df = pd.DataFrame(data = columns_missing_info, columns = column_names)

    return missing_info_df

missing_train_df = missing_train_info(df)

pd.set_option("display.max_rows", None, "display.max_columns", None)

missing_train_df.head(df.shape[1])

"""## Exploratory Data Analysis"""

ax = sns.countplot(x='isFraud', data = df)

plt.title("\nDistribution of Class Labels\n")

plt.margins(0.05, 0.1)

for p in ax.patches:
  x=p.get_bbox().get_points()[:,0]
  y=p.get_bbox().get_points()[1,1]
  ax.annotate('{} ({:.1f}%)'.format(int(y),100.*y/len(df)), (x.mean(), y),
          ha='center', va='bottom')

plt.show()

"""> This shows that the dataset is highly imbalanced"""

plt.figure(figsize=(10,8))
plt.pie(df.type.value_counts().values,labels=df.type.value_counts().index,  autopct='%.0f%%')
plt.title("Transaction Type")
plt.show()

ax = sns.countplot(x='type', hue='isFraud', data = df)

plt.margins(0.05, 0.1)

for p in ax.patches:
  x=p.get_bbox().get_points()[:,0]
  y=p.get_bbox().get_points()[1,1]
  ax.annotate('{:.1f}%'.format(100.*y/len(df)), (x.mean(), y),
          ha='center', va='bottom')

plt.title('\nTransaction type Count Plot\n')
plt.show()

fraud_size=df[df['isFraud'].isin([1])].groupby(['type']).size()
fraud_size

"""> PAYMENT, DEBIT and CASH_IN has no fraudulent transaction"""

plt.figure(figsize=(10,8))
plt.pie(fraud_size.values,labels=fraud_size.index, autopct='%.6f%%')
plt.title("Transaction Type Distribution")
plt.show()

transfer_fraud = df[((df['type']=='TRANSFER') & df['isFraud']==1)]
transfer_fraud['nameOrig'].value_counts()

cash_out_fraud = df[(df['type'] == 'CASH_OUT') & (df['isFraud'] == 1)]
cash_out_fraud['nameDest'].value_counts()

fraud_trans = df[df['isFraud'] == 1]
valid_trans = df[df['isFraud'] == 0]

trans_transfer = df[df['type'] == 'TRANSER']
trans_cashout = df[df['type'] == 'CASH_OUT']

print('Has the receiving account used for cashing out?')
trans_transfer.nameDest.isin(trans_cashout.nameOrig).any()

df_temp = df[df.isFlaggedFraud==1]
print("How many frauds transactions are Flagged?:")
print(len(df_temp))

# Check if df_temp is empty before proceeding
if df_temp.empty:
    print("No fraud transactions are flagged.")
else:
    print("What type of transactions are they?")
    print(df_temp['type'].value_counts().index[0])
    print("Are all these flagged also marked as Fraud Transactions?")
    print(df_temp['isFraud'].value_counts()[1] == len(df_temp))

df[['amount']].describe()

sns.histplot(x='amount', hue='isFraud', data=df, kde=True)
plt.title('\nTransactionAmt Distribution Plot\n')
plt.show()

def check_corr(lst):
  '''
    Utility Function to check Correlation between features.
  '''
  df_corr= df[lst].corr()
  plt.figure(figsize= (25,15))
  sns.heatmap(df_corr,annot= True)
  plt.show()

check_corr(['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest','isFraud','isFlaggedFraud'])

def label_encode(df, catf):

  '''
    Utility Function to Encode Categorical Features.
  '''

  for f in catf:

    df[f] = df[f].astype(str)

    le = LabelEncoder()
    le.fit(df[f])
    mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    df[f] = le.transform(df[f])


  return (df)

data = df.copy()

catf = ['type','nameOrig','nameDest']

data = label_encode(data,catf)

"""## Baseline Performance

#### Hyperparameter Tuning
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Handle missing values in 'isFraud' column before splitting
data['isFraud'] = data['isFraud'].fillna(data['isFraud'].mode()[0])  # Fill with mode

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(['isFraud', 'isFlaggedFraud'], axis=1),
    data['isFraud'],
    test_size=0.3,
    random_state=42,
    stratify=data['isFraud']
)

def hyperparam_vs_auc(train_roc_auc, cv_roc_auc):

    '''
        Utility Function to plot the Training and Cross Validation ROC-AUC Values
        for different Hyperparameter.
    '''

    plt.plot(range(len(train_roc_auc)), train_roc_auc, label='Train AUC')
    plt.plot(range(len(cv_roc_auc)), cv_roc_auc, label='CV AUC')

    plt.scatter(range(len(train_roc_auc)), train_roc_auc, label='Train AUC points')
    plt.scatter(range(len(cv_roc_auc)), cv_roc_auc, label='CV AUC points')

    plt.xticks(range(len(train_roc_auc)))
    plt.legend()
    plt.xlabel("Hyperparameter Index")
    plt.ylabel("AUC")
    plt.title("\n Hyperparameter vs ROC-AUC \n")
    plt.grid()
    plt.show()

learning_rate = [2e-2, 3e-1, 1e-1]
max_depth = [8, 12, 16]
subsample = [0.6,0.8,1]
colsample_bytree = [0.6,0.8,1]

results = {}

dtrain = xgb.DMatrix(X_train, label=y_train)

for rate in learning_rate:
  for depth in max_depth:
    for sample in subsample:
      for colsample in colsample_bytree:

        params = {
          'objective' : 'binary:logistic',
          'eval_metric' : 'auc',
          'learning_rate' : rate,
          'max_depth' : depth,
          'subsample' : sample,
          'colsample_bytree' : colsample,
          'tree_method' : 'gpu_hist',
          'random_state' : 3,
        }

        history = xgb.cv(
            params,
            dtrain,
            num_boost_round = 3000,
            nfold = 3,
            metrics ='auc',
            early_stopping_rounds = 100,
            verbose_eval=100,
            seed=3,
            shuffle = False
        )

        name = "learning_rate : "+str(rate)+" max_depth : "+str(depth)+" subsample : "+str(sample)+" colsample_bytree : "+str(colsample)
        results[name] = (history.iloc[-1]['train-auc-mean'],history.iloc[-1]['test-auc-mean'])

train_roc_auc = [auc[0] for auc in results.values()]
cv_roc_auc = [auc[1] for auc in results.values()]

plt.figure(figsize=(25,8))
hyperparam_vs_auc(train_roc_auc, cv_roc_auc)

best_params = list(results.keys())[12]

print("The Best Paramaters are {} ".format(best_params))

def tpr_fpr_threshold_auc(Y, proba):

    '''
        Utility Function to return fpr, tpr, threshold and roc_auc.
    '''

    fpr, tpr, threshold = sklearn.metrics.roc_curve(Y, proba)
    roc_auc = metrics.auc(fpr, tpr)

    return (fpr, tpr, threshold, roc_auc)

def train_cv_auc(train_fpr, train_tpr, train_roc_auc, cv_fpr, cv_tpr, cv_roc_auc):

    '''
        Utility Function to Draw ROC-AUC Curves for Train and Validation Datasets.
    '''

    plt.title('\nReceiver Operating Characteristic\n')
    plt.plot(train_fpr, train_tpr, label = 'Train AUC = %0.2f' % train_roc_auc)
    plt.plot(cv_fpr, cv_tpr, label = 'CV AUC = %0.2f' % cv_roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate (TPR)')
    plt.xlabel('False Positive Rate (FPR)')
    plt.show()

def best_threshold(threshold,tpr,fpr):

    '''
        Utility Function to return the best threshold value based on the TPR and FPR values.
    '''

    return threshold[np.argmax(tpr*(1-fpr))];


def class_label(best_thr, pred_score):

    '''
        Utility Function to return the class label based on the chosen threshold.
    '''
    return np.where(pred_score>best_thr,1,0);

def draw_confusion_matrix(Y, predicted):

    '''
        Utility Function to draw Confusion Matrix.
    '''

    cv_cm = metrics.confusion_matrix(Y, predicted)
    sns.heatmap(cv_cm, annot=True,fmt="d",cmap='Oranges', xticklabels=['Legit', 'Fraudulent'], yticklabels=['Legit', 'Fraudulent'])
    plt.title('\nConfusion  Matrix\n')
    plt.xlabel('True')
    plt.ylabel('Predicted')
    plt.show()

"""### Tuned Prediction"""

X = data.drop(['isFraud', 'isFlaggedFraud'], axis=1)
y = data['isFraud']

X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.20, shuffle=False)
del X, y

print("*"*45)
print("\n Train Data Shape : {} \n".format(X_train.shape))
print("\n Cross Validation Data Shape : {} \n".format(X_cv.shape))
print("*"*45)

# Import necessary library
import xgboost as xgb

# Convert data to DMatrix format for XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_cv, label=y_cv)

# Set parameters for XGBoost
param = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'tree_method': 'gpu_hist',
    'random_state': 3,
    'subsample': 0.8,
    'max_depth': 12,
    'colsample_bytree': 0.6,
    'learning_rate': 0.02
}

# Train the model with early stopping
num_round = 3000
early_stopping_rounds = 100
watchlist = [(dtrain, 'train'), (dval, 'eval')]
results = xgb.train(param, dtrain, num_round, evals=watchlist,
                    early_stopping_rounds=early_stopping_rounds, verbose_eval=50)

# Access the trained model
clf = results

plt.rcParams["figure.figsize"] = (14, 20)
xgb.plot_importance(clf,max_num_features=50, grid=False, height=0.4)
plt.show()

# Import necessary library
import xgboost as xgb

# ... (Your existing code)

# Predict probabilities using DMatrix
dtrain_pred = xgb.DMatrix(X_train)
dcv_pred = xgb.DMatrix(X_cv)
dtest_pred = xgb.DMatrix(X_test)

train_proba = clf.predict(dtrain_pred)
cv_proba = clf.predict(dcv_pred)
test_proba = clf.predict(dtest_pred)

train_fpr, train_tpr, train_threshold, train_roc_auc = tpr_fpr_threshold_auc(y_train, train_proba)
cv_fpr, cv_tpr, cv_threshold, cv_roc_auc = tpr_fpr_threshold_auc(y_cv, cv_proba)

plt.rcParams["figure.figsize"] = (6, 5)
train_cv_auc(train_fpr, train_tpr, train_roc_auc, cv_fpr, cv_tpr, cv_roc_auc)

best_thr = best_threshold(cv_threshold, cv_tpr, cv_fpr)
cv_pred = class_label(best_thr, cv_proba)

draw_confusion_matrix(y_cv, cv_pred)

"""## Feature Engineering"""

def compute_vif(df, features):
    data = df[features]
    vif = {column:variance_inflation_factor(data.values, idx) for idx, column in enumerate(features)}
    return vif

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

def compute_vif(df, features):
    data = df[features]
    # Handle infinite values
    data = data.replace([np.inf, -np.inf], np.nan)
    # Fill or drop missing values
    data = data.dropna()  # Or use data.fillna(some_value)
    vif = {column:variance_inflation_factor(data.values, idx) for idx, column in enumerate(features)}
    return vif

# Define the features you want to analyze
features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig'] # Replace with your actual column names

# Now call the function
vif = compute_vif(df, features)
print(vif)

vif

def cat_num_features(df):

    '''
        Utility Function to get the names of Categorical Features and
        Numerical Features of the given Dataset.
    '''

    catf = []
    numf = []


    catf = ['type','nameOrig', 'isFraud'
            ,'nameDest','isFlaggedFraud']



    catf = [feature for feature in catf if feature in df.columns.values]
    numf = [feature for feature in df.columns if feature not in catf and not feature == 'isFraud']

    return (catf, numf)

def covariate_shift(df, feature, catf):

    '''
        Utility Function to compute Covariate Shift for given Feature in a Dataset.
    '''

    data = df.copy()
    data = label_encode(data, catf)

    X_train, X_test, y_train, y_test = train_test_split(data[feature], data['isFraud'], test_size=0.33,
                                                        random_state=3, stratify=data['isFraud'])
    clf = XGBClassifier(
      objective='binary:logistic',
      eval_metric='auc',
      n_estimators=500,
      tree_method='gpu_hist',
      random_state=3,
    )

    clf.fit(X_train.values.reshape(-1,1), y_train.values, verbose=1)

    roc_auc =  roc_auc_score(y_test.values, clf.predict_proba(X_test.values.reshape(-1,1))[:, 1])

    del X_train, y_train, X_test, y_test

    return roc_auc

def adversarial_validation(df, catf):
    list_auc_value = []

    cols = list(df.columns)
    cols.remove('isFraud')

    for f in cols:

        auc = covariate_shift(df, f, catf)
        list_auc_value.append(auc)
        print('*'*45)
        print('feature:', f, 'covariate shift:', auc)
        print('*'*45)

    cov = pd.Series(list_auc_value, index = cols).sort_values()

    return cov

catf, numf = cat_num_features(df)

def covariate_shift(df, feature, catf):

    '''
        Utility Function to compute Covariate Shift for given Feature in a Dataset.
    '''

    data = df.copy()
    data = label_encode(data, catf)  # Assuming 'label_encode' is defined elsewhere

    # Handle potential issue with stratification
    if len(data['isFraud'].unique()) < 2:
        print

new_type = {'PAYMENT':'OTHERS', 'TRANSFER':'TRANSFER', 'CASH_OUT':'CASH_OUT', 'DEBIT':'OTHERS', 'CASH_IN':'OTHERS'}
df['type']=df['type'].map(new_type)
df['type'].unique()

df['type2'] = np.nan
df.loc[df.nameOrig.str.contains('C') & df.nameDest.str.contains('C'), 'type2'] = 'CC'
df.loc[df.nameOrig.str.contains('C') & df.nameDest.str.contains('M'), 'type2'] = 'CM'
df.loc[df.nameOrig.str.contains('M') & df.nameDest.str.contains('C'), 'type2'] = 'MC'
df.loc[df.nameOrig.str.contains('M') & df.nameDest.str.contains('C'), 'type2'] = 'MM'

df.drop(columns = ['nameOrig','nameDest','step'], axis = 'columns', inplace = True)

df = pd.get_dummies(df, prefix = ['type', 'type2'], drop_first = False)

# Check for missing values in 'isFraud' column
print(df['isFraud'].isnull().sum())

# Handle missing values (e.g., drop rows with missing values)
df.dropna(subset=['isFraud'], inplace=True)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df.drop(['isFraud','isFlaggedFraud'], axis=1),
    df['isFraud'],
    test_size=0.33,
    random_state=42,
    stratify=df['isFraud']
)

"""## Final Hyperparameter Tuning"""

learning_rate = [2e-2, 3e-1, 1e-1]
max_depth = [8, 12, 16]
subsample = [0.6,0.8,1]
colsample_bytree = [0.6,0.8,1]

results = {}

dtrain = xgb.DMatrix(X_train, label=y_train)

for rate in learning_rate:
  for depth in max_depth:
    for sample in subsample:
      for colsample in colsample_bytree:

        params = {
          'objective' : 'binary:logistic',
          'eval_metric' : 'auc',
          'learning_rate' : rate,
          'max_depth' : depth,
          'subsample' : sample,
          'colsample_bytree' : colsample,
          'tree_method' : 'gpu_hist',
          'random_state' : 3,
        }

        history = xgb.cv(
            params,
            dtrain,
            num_boost_round = 3000,
            nfold = 3,
            metrics ='auc',
            early_stopping_rounds = 100,
            verbose_eval=100,
            seed=3,
            shuffle = False
        )

        name = "learning_rate : "+str(rate)+" max_depth : "+str(depth)+" subsample : "+str(sample)+" colsample_bytree : "+str(colsample)
        results[name] = (history.iloc[-1]['train-auc-mean'],history.iloc[-1]['test-auc-mean'])

train_roc_auc = [auc[0] for auc in results.values()]
cv_roc_auc = [auc[1] for auc in results.values()]

plt.figure(figsize=(25,10))
hyperparam_vs_auc(train_roc_auc, cv_roc_auc)

best_params = list(results.keys())[3]
print("The Best Paramaters are {} ".format(best_params))

"""## Final Modeling"""

X = df.drop(['isFraud','isFlaggedFraud'], axis=1)
y = df['isFraud']

X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.20, shuffle=False)
del X, y

clf = XGBClassifier(
    objective='binary:logistic',
    eval_metric='auc',
    n_estimators=3000,
    tree_method='gpu_hist',
    random_state=3,
    subsample=0.8,
    max_depth=8,
    colsample_bytree=0.6,
    learning_rate=0.02
)

#

# Previous code to define and configure the classifier (unchanged)
clf = XGBClassifier(
    objective='binary:logistic',
    eval_metric='auc',
    n_estimators=3000,
    tree_method='gpu_hist',
    random_state=3,
    subsample=0.8,
    max_depth=8,
    colsample_bytree=0.6,
    learning_rate=0.02
)

# Fit the model to your training data
clf.fit(X_train, y_train)

# Now you can plot the feature importances
plt.rcParams["figure.figsize"] = (14, 20)
xgb.plot_importance(clf,max_num_features=50, grid=False, height=0.4)
plt.show()

train_proba = clf.predict_proba(X_train)[:,1]
cv_proba = clf.predict_proba(X_cv)[:,1]

train_fpr, train_tpr, train_threshold, train_roc_auc = tpr_fpr_threshold_auc(y_train, train_proba)
cv_fpr, cv_tpr, cv_threshold, cv_roc_auc = tpr_fpr_threshold_auc(y_cv, cv_proba)

plt.rcParams["figure.figsize"] = (6, 5)
train_cv_auc(train_fpr, train_tpr, train_roc_auc, cv_fpr, cv_tpr, cv_roc_auc)

best_thr = best_threshold(cv_threshold, cv_tpr, cv_fpr)
cv_pred = class_label(best_thr, cv_proba)

draw_confusion_matrix(y_cv, cv_pred)